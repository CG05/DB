{"cells":[{"cell_type":"markdown","metadata":{},"source":["## lightgbm 패키지 설치\n","```\n","pip install lightgbm\n","```"]},{"cell_type":"markdown","metadata":{"id":"aZWgiVAQQ5me"},"source":[">튜닝"]},{"cell_type":"markdown","metadata":{"id":"6NfYM-RsOYuH"},"source":["> min_child_samples는 최종 결정 클래스인 Leaf Node가 되기 위해서 최소한으로 필요한 데이터 개체의 수를 의미하며, 과적합을 제어하는 파라미터이다. 이 파라미터의 최적값은 훈련 데이터의 개수와 num_leaves에 의해 결정된다. 너무 큰 숫자로 설정하면 예측률이 떨어지는 과소적합(under-fitting)이 일어날 수 있으며, 아주 큰 데이터셋이라면 적어도 수백~수천 정도로 가정하는 것이 편리하다\n","\n","> num_leaves는 개별 트리가 가질 수 있는 최대 리프의 개수이고 LightGBM 모델의 복잡도를 제어하는 주요 파라미터이다. 일반적으로 계수를 높이면 정확도가 올라가지만 트리의 깊이가 깊어지고 모델이 복잡도가 커져 과적합이 될 가능성이 높다.\n","\n","> reg_alpha, reg_lambda는 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다."]},{"cell_type":"markdown","metadata":{"id":"tIdIcafegew0"},"source":["> 결국 다음과 같이 파라미터를 설정하는 것이 예측률을 높일 수 있다.\n","- min_child_samples= 10, num_leaves= 5, reg_alpha= 0.05, reg_lambda= 0.25"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"10. Light GBM 실습.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
